<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>She is The Music | Juliette Mangon</title>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet" />
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Instrument+Sans:ital,wght@0,400..700;1,400..700&display=swap"
    rel="stylesheet" />
  <link rel="stylesheet" href="site.css" />


  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    html,
    body {
      height: 100%;
    }

    body {
      background: var(--bg);
      color: var(--text);
      font-family: "Inter", sans-serif;
      line-height: 1.6;
      font-size: 1.05rem;
      overflow-x: hidden;
    }


    main {
      max-width: 920px;
      margin: 6rem auto 3rem;
      padding: 0 1.25rem;
      animation: slideIn 0.6s ease forwards;
      opacity: 0;
      transform: translateY(30px);
    }

    @keyframes slideIn {
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    h2 {
      font-size: 2.5rem;
      font-weight: 500;
      margin: 1.2rem 0 0.6rem;
    }

    .highlightname {
      color: var(--highlight);
      background: var(--text2);
      text-decoration: underline;
    }

    a {
      color: var(--texxt);
      text-decoration: none;
      transition: color 0.1s;
    }

    a:hover {
      background-color: var(--highlight);
      color: var(--bg);
    }

    p {
      margin-bottom: 1.5rem;
      font-size: 1.45rem;
    }

    .photos {
      margin-top: 2rem;
      display: flex;
      gap: 0.5rem;
      flex-wrap: wrap;
    }

    .photos img {
      width: 100%;
      max-width: 530px;
      height: auto;
      object-fit: cover;
      transition: transform 0.3s;
      flex: 1 1 48%;
    }

    @media (max-width: 900px) {
      .topnav {
        position: absolute;
        top: 0.75rem;
        left: 0.75rem;
        right: auto;
        display: flex;
        flex-direction: column;
        align-items: flex-start;
        gap: 0.25rem;
        z-index: 1500;
      }

      .topnav a {
        display: inline-block;
        padding: 0.125rem 0;
        line-height: 1.2;
      }

      #themeToggle {
        position: absolute;
        top: 0.75rem;
        right: 0.75rem;
        width: 32px;
        height: 32px;
        z-index: 2000;
      }

      main {
        margin-top: 10rem;
        padding-left: 1rem;
        padding-right: 1rem;
      }

      .contact-columns {
        grid-template-columns: 1fr;
      }

      .photos img {
        flex: 1 1 100%;
      }
    }

    @media (max-width: 1200px) and (min-width: 901px) {
      .topnav {
        gap: 0.75rem;
      }

      main {
        margin-top: 6rem;
        padding-left: 1.25rem;
        padding-right: 1.25rem;
      }
    }
  </style>
</head>

<body>
  <div id="themeToggle" title="Switch Theme"></div>
  <nav class="topnav" aria-label="Site">
    <a href="index.html">home</a>
    <a href="about.html">about</a>
    <a href="resume.html">resume</a>
    <a href="paintings.html">paintings</a>
    <a href="lists.html">lists</a>
    <a href="contact.html">contact</a>
  </nav>

  <main>
    <h2>Music metadata analytics tool</h2>

    <p>
      I began developing this tool during my time as a music tech mentee in
      <a href="https://sheisthemusic.org/" target="_blank">
        <span class="highlightname">She Is The Music</span>
      </a>'s Connect TogetHER mentorship program.
      What started as an exploration of music collaboration networks has since evolved into a focused
      metadata analytics product for understanding who actually shapes the sound of modern music.
    </p>

    <p>
      The core idea is simple: many creative decisions in the music industry are driven by taste,
      intuition, and pattern recognition, yet the data needed to inspect those patterns is fragmented
      across platforms and difficult to analyze at scale.
      This tool aggregates and normalizes music metadata to surface contributor-level signals that are
      otherwise invisible when looking at songs or artists in isolation.
    </p>
    </section>

    <section>
      <h2>The problem</h2>

      <p>
        Music discovery and hiring decisions are often made by listening rather than by reading credits.
        A playlist might have a cohesive sound or emotional through-line that is difficult to articulate,
        yet highly consistent. Translating that intuition into actionable insight requires hours of manual
        research across Spotify, liner notes, label databases, and third-party sites.
      </p>

      <p>
        Credits data is also structurally fragmented. Contributors appear under different naming
        conventions across platforms, roles are inconsistently labeled, and there is no single place to
        view how frequently a given songwriter, producer, or musician appears across a body of work.
        As a result, contributor-level patterns remain hidden, even though they often explain why a set
        of songs feels connected.
      </p>
    </section>

    <section>
      <h2>Primary use case</h2>

      <p>
        A representative use case is talent discovery through contributors rather than artists.
        Suppose a music team has a playlist of 100 songs that share a particular sound they cannot quite
        describe. Individually, the songs span different artists, labels, and release years, but
        collectively they feel cohesive.
      </p>

      <p>
        By ingesting the playlist into the system, the tool aggregates metadata for each track and
        resolves all contributors across the dataset. When contributors are ranked by frequency, clear
        patterns often emerge: the same songwriter appears on a large subset of tracks, a specific
        drummer or producer recurs across unrelated artists, or a small group of collaborators forms a
        dense creative cluster.
      </p>

      <p>
        This allows teams to move from intuition to evidence. Instead of hiring based solely on artist
        adjacency or genre labels, they can identify the individuals most responsible for shaping the
        sound they are responding to, and make informed hiring, staffing, or collaboration decisions.
      </p>
    </section>

    <section>
      <h2>What the system does</h2>

      <p>
        The tool ingests song-level metadata and surfaces contributor-level insights across an arbitrary
        collection of tracks. All contributors exposed by the underlying APIs are included, meaning the
        system is as expressive as the available metadata allows.
      </p>

      <p>
        Contributors are ranked by frequency of appearance across the input set, enabling quick
        identification of recurring creative roles. The system does not infer taste, quality, or success;
        it strictly analyzes metadata relationships and recurrence patterns.
      </p>

      <p>
        The result is a structured view of who is involved, how often they appear, and how contributors
        relate to one another across songs, artists, and labels.
      </p>
    </section>

    <section>
      <h2>How it works</h2>

      <p>
        At a high level, the system follows a pipeline of ingestion, normalization, aggregation, and
        analysis.
      </p>

      <p>
        Song data is first retrieved using the Spotify API as an entry point, then enriched using
        MusicBrainz and Discogs to gather detailed credits and release metadata. Contributor names are
        normalized to reduce duplication caused by formatting or platform-specific differences.
      </p>

      <p>
        All metadata is stored in a MongoDB database, allowing the system to persist historical inputs,
        recompute insights across larger datasets, and generate contributor statistics efficiently.
        Contributors are ranked by frequency across the selected corpus, and derived insights are
        generated directly from these counts.
      </p>

      <p>
        The frontend presents both raw metadata and aggregated insights, making it possible to inspect
        individual songs while also understanding broader contributor-level patterns.
      </p>
    </section>

    <section>
      <h2>Technical architecture</h2>

      <p>
        The frontend is built in Next.js and is responsible for user input, data visualization, and
        presenting derived insights. The backend is an Express application that orchestrates API calls,
        handles normalization logic, and manages persistence.
      </p>

      <p>
        The system integrates the Spotify API, MusicBrainz API, and Discogs API. Only metadata is used;
        no audio features or machine learning models are involved at this stage. Contributor ranking is
        based on frequency rather than probabilistic inference, by design, to keep results interpretable
        and verifiable.
      </p>

      <p>
        The application is deployed on Render. Access requires API credentials for the integrated
        services.
      </p>
    </section>

    <section>
      <h2>Limitations and next steps</h2>

      <p>
        The system is bounded by the quality and completeness of available metadata. Contributor matching
        is only as accurate as the upstream data sources, and discrepancies across platforms can still
        introduce noise.
      </p>

      <p>
        Future iterations could incorporate more robust entity resolution, role-specific weighting,
        temporal analysis, or integration with additional rights and credits databases. The current
        version intentionally prioritizes transparency and traceability over predictive modeling.
      </p>
    </section>

    <section>
      <h2>Running the project</h2>

      <p>
        The application requires valid API credentials for Spotify, MusicBrainz, and Discogs.
        To run locally, users must create developer accounts for these services, configure environment
        variables, and start both the Express backend and the Next.js frontend.
      </p>

      <p>
        Full setup instructions and source code are available in the project repository.
      </p>
    </section>


    </section>

    <section>
      <div class="photos"></div>
    </section>
    <footer>
      <p style="
            text-align: center;
            font-size: 0.9rem;
            color: var(--text);
            margin-bottom: 0.7rem;
            margin-top: 1rem;
            padding-bottom: 2rem;
          ">
        Â© 2025 Juliette Mangon
      </p>
    </footer>
  </main>


  <script>
    // THEME
    const themes = {
      default: {
        "--bg": "#eedae5ff",
        "--text": "#ff6200",
        "--text2": "#ff6200",
        "--highlight": "#c9f5ffff",

      },
      alt: {
        "--bg": "#d2e6ff",
        "--text": "#A70A4E",
        "--text2": "#A70A4E",
        "--highlight": "#ff85c2ff ",
      },
      gray: {
        "--bg": "#c3d7daff",
        "--text": "#4c354aff",
        "--text2": "#4c354aff",
        "--highlight": "#f28bdaff",
      },
      blue: {
        "--bg": "#2d6d9eff",
        "--text": "#cee1ffff",
        "--text2": "#cee1ffff",
        "--highlight": "#c82302ff",
      },
      beige: {
        "--bg": "#F5F1E8",
        "--text": "#2A2A2A",
        "--text2": "#2A2A2A",
        "--highlight": "#dbb6caff",

      }
    };
    const order = ["blue", "default", "alt", "gray", "beige"];
    function applyTheme(name) {
      if (!themes[name]) name = "default";
      for (const [k, v] of Object.entries(themes[name])) {
        document.documentElement.style.setProperty(k, v);
      }
      localStorage.setItem("theme", name);
    }
    applyTheme(localStorage.getItem("theme") || "default");

    document.getElementById("themeToggle").addEventListener("click", () => {
      const current = localStorage.getItem("theme") || "default";
      const idx = (order.indexOf(current) + 1) % order.length;
      applyTheme(order[idx]);
    });
  </script>
</body>

</html>